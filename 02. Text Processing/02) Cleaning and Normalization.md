## 02) 정제(Cleaning) and 정규화(Normalization)

- 토큰화(tokenization): 코퍼스에서 용도에 맞게 토큰을 분류하는 작업
  - 토큰화 작업 전, 후로 정제/정규화가 늘 따라붙는다.

- 정제(cleaning): 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
- 정규화(normalization): 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦

보통 정제 작업은 토큰화 이전에 이루어지지만, 토큰화 이후에도 남아있는 노이즈들을 제거하기 위해서 지속적으로 이루어지도 한다.

### 1. 규칙에 기반한 표기가 다른 단어들의 통합
필요에 따라 코딩을 통해 정규화 규칙을 정의할 수 있다. 즉, 직접 코딩을 통해 표기가 다른 단어들을 통합하는 규칙을 만들 수 있다는 것.
좋은 예시로 USA와 US가 있다. 또는 "uh-huh"와 "uhhuh"가 있을 것이다.

- 표기가 다른 단어들을 통합하는 방법
  - 어간 추출(stemming)
  - 표제어 추출(lemmatizaiton)

### 2. 대, 소문자 통합
영어권에서는 소문자만으로도 충분히 본래의 의미를 해치지 않고 해석하는 것이 가능하다.
그렇기에 영어권에서는 대개 소문자로 통합이 이루어지는 편이다.

물론 모든 경우에 소문자 통합이 유효한 것은 아니고, 미국을 뜻하는 US나 우리를 뜻하는 us는 대소문자로 구분되어야 한다.
또한, 사람 이름이나 대명사 역시 대문자로 작성되는 것이 옳고, 또 의미를 지니고 있다.

이건 연구자가 수집한 코퍼스에 따라서 실제로 천차만별일 수는 있으나,
"결국에는 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도"한다.

### 3. 불필요한 단어의 제거
- 노이즈 데이터(noise data)
  - 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)
  - 분석하고자 하는 목적에 맞지 않는 불필요 단어들


- 불필요 단어들을 제거하는 방법
  - 불용어 제거
  - 등장 빈도가 적은 단어 제거
  - 길이가 짧은 단어 제거

#### (1) 등장 빈도가 적은 단어
텍스트 데이터에서 너무 적게 등장하는 단어라면 자연어 처리에 도움이 되지 못한다.
교재에서는 예시로 100000개의 메일을 가지고 스팸 분류기를 만드는 상황을 들었다.
만약 100000건 중, 다섯번 밖에 등장하지 않는 단어라면 오히려 제거하는 것이 분류에 도움이 된다.

#### (2) 길이가 짧은 단어
길이가 짧은 단어를 삭제하는 방식은 영어권에서 효과적이다.
왜냐하면 "영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당"하기 때문이다.
부가적인 이유로 "단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함"도 있다.

**그러나 과연 길이가 짧은 단어를 삭제하는 방식이 한국어에서도 효과적일까?**

한국어는 영어에 비해 짧은 길이의 단어로 함축적인 의미를 담아낸다. 그렇기에 한국어에서는 이런 방식이 효과적이지 않을 수 있다.

아무튼 본론으로 돌아와서, 만약 영어에서 길이가 1인 단어를 제거한다면 관사 'a'와 주어로 쓰이는 'I'가 제거된다.
비슷하게 길이가 2인 단어를 제거한다면 it, at, to, on, in, by 등과 같은 대부분 불용어에 해당하는 단어들이 제거된다고 한다.

다만 길이가 3인 단어부터는 car 등 의미를 가지고 있는 경우가 많아 잘 사용하지는 않는다.

### 4. 정규 표현식(Regular Expression)
뉴스기사와 같이 반복적인 노이즈를 잡아낼 수만 있다면, 정규 표현식을 통해 제거할 수 있는 경우가 많다.
