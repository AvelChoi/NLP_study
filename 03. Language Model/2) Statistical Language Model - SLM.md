## 2) 통계적 언어 모델
전통적인 접근 방법이며, Statistical Language Model을 줄여서 SLM이라 부른다.

### 1. 조건부 확률
사회조사분석사부터 경영통계학까지, 조건부 확률은 꾸준히 얼굴을 비춘다.

[확률 용어 정리 - 조건부확률](https://velog.io/@du-du-zi/%ED%99%95%EB%A5%A0-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-%EC%A1%B0%EA%B1%B4%EB%B6%80%ED%99%95%EB%A5%A0)

``p(B|A) = P(A, B) / P(A)``
``A하에서 B일 확률 = A와 B가 교집합일 확률 / A일 확률``

위 식을 계산하기 위해서는 확률의 곱셈법칙도 필요하다.

``P(A, B) = P(A) * P(B|A)``
``A와 B의 교집합일 확률 = A일 확률 * A 하에서 B일 확률``

더 많은 확률들이 모이면 어떻게 될까? 조건부 확률의 연쇄법칙이 있다.

``P(x1, x2, x3, ..., xn) = P(x1) * P(x2|x1) * P(x3|x1, x2) * ... P(xn|x1, ..., xn-1)``

### 2. 문장에 대한 확률
위에서는 n개의 x들로 조건부 확률을 정의했다. 그렇다면 n개의 단어들이 있는 문장으로 x를 대체해보면 어떨까?
> An adorable little boy is spreading smiles

예시로 제시된 문장은 다음과 같다. 이를 확률로 나타내는 것까지는 간단하다.

``P(An adorable little boy is spreading smiles)``

문장에서 하나의 단어들은 아무런 이유 없이 그 자리를 차지하고 있지 않는다.
마치 사회의 관습, 문화처럼 각 단어들로 환원될 수 없는 무언가가 더 있기에 문장이 완성된다.
그렇기에 "문장의 확률을 구하고자 조건부 확률을 사용"하는 것이다.

> 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 **문장의 확률**은 **각 단어들이 이전 단어**가 주어졌을 때 **다음 단어로 등장할 확률의 곱**으로 구성됩니다.

아까 전에 보았던 곱집합이 다시 등장한다. 조건부 확률과 곱집합이 합쳐진 수식으로, n=1부터 n까지 각각의 wn의 조건부 확률을 모두 곱한 셈이 된다.
최종적으로 각 단어들에 대입해보면 다음과 같다.

``P(An adorable little boy is spreading smiles) =
P(An) * P(adorable|An) * P(little|An adorable) * P(boy|An adorable little) *
... * (smiles|An adorable little boy is spreading)
``

중간에 생략된 부분이 있지만, 이런 형태로 조건부 확률의 연쇄 법칙을 활용한다.

### 3. 카운트 기반의 접근
위의 방법을 실제로 SLM이 어떻게 활용하고 있을까? SLM은 "카운트에 기반하여 확률을 계산"한다.

``P(is|An adorable little boy) = count(An adorable little boy is) / count(An adorable little boy)``

"An adorable little boy is"가 30번 등장했고, "An adorable little boy"가 100번 등장했다고 가정해보자.
이때 위 수식을 따라간다면 ``30 / 100 = 0.3 = 30%`` is가 등장할 조건부 확률은 30%가 된다.

### 4. 카운트 기반 접근의 한계 - 희소 문제
여기서 통계의 근본적인 문제가 등장한다. 바로 '근사'다.
통계에서 사용하는 여러 기법들은 주로 표본집단을 통해 모집단을 이해하려 노력한다.
중요한 점은 정확히 모집단과 동일한 값을 알아낼 수가 없다는 것이다. 따라서 최대한 근접한 값을 주로 찾게 된다.
자연어처리 역시 마찬가지다.

> 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표

즉, 미리 확보된 코퍼스(표본집단)를 통해 현실세계에서 사람들이 말하는 바(모집단)에 가까이 다가가고자 한다.
그러나 모집단을 전수조사 하는 것은 사실상 불가능에 가깝다. 마찬가지로 코퍼스를 수집하는 데에도 한계가 있을 수밖에 없다.

만약 수집한 말뭉치에서 "An adorable little boy"라는 문장이 없다면 분모가 0이 되어버리므로 수식 자체가 성립하지 않는다.
현실에서는 충분히 성립할 수 있는 문장인데도 불구하고 말이다. 통계라면 이를 제2종 오류라고 정리했을 것이다. 

``H0: An adorable little boy 다음에는 is가 나오지 않는다.``

``H1: An adorable little boy 다음에는 is가 나온다.``

귀무가설이 거짓임에도 불구하고 이를 기각하는데에 실패했으므로 제2종 오류에 해당한다. ~~그냥 데이터 수집을 부실하게 한건 아니고?~~

아무튼 희소 문제는 다음과 같이 정의할 수 있다.

> 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제

이를 해결하기 위한 방법으로 n-gram 언어 모델(본 교재에서 다룸), 스무딩, 백오프와 같은 "일반화" 기법들이 있다.
그러나 희소 문제에 대한 근본적인 해결책을 제시하지 못하고, 통계적 언어 모델에서 인공 신경망 모델로 넘어간다.