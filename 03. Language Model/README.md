# 03. 언어 모델
> 언어 모델(Languagel Model)이란 단어 시퀀스(문장)에 확률을 할당하는 모델을 의미

> 이번 챕터에서는 통계에 기반한 전통적인 언어 모델(Statistical Languagel Model, SLM)에 대해서 학습

통계 기반한 언어 모델은 자연스러운 자연어를 근사(표현하기에, 접근하기에) 많은 한계가 있었다고 한다.
또한, 인공신경망이 한계를 해결하면서 사용 빈도는 줄어들었지만 여전히 통계 기반 방법론에 대한 이해는
언어 모델에 대한 전체적인 시야를 갖는데 도움이 된다.

아직 본격적인 학습을 시작하기 전이지만, 지금까지 쌓아 올린 통계적 지식은 사회과학에서 사용하는 기법들이었다.
특히 사회조사분석사 2급 정도의 내용을 배우고, 전공 수업을 들으며 작성한 설문지를 바탕으로 SPSS를 사용해본 것이 전부였는데 자연어 처리에서는 어떤 방법을 사용할지 궁금하다.

## 1) 언어모델이란?
### 1. 언어 모델
> 언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델
> 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록...

즉 언어 모델링은 "주어진 단어들로부터 아직 모르는 단어를 예측하는 작업"을 뜻한다.

### 2. 단어 시퀀스의 확률 할당
예시로는 P(나는 버스를 탔다)와 P(나는 버스를 태운다)가 제시되었는데, 버스를 탔다가 더 자연스러운 표현이다.
사람이야 자연스러운/부자연스러운 문장을 판별할 수 있지만, 기계는 그렇지 않기에 확률로 "보다 적절한 문장을 판단"한다.

### 3. 주어진 이전 단어들로부터 다음 단어 예측하기
다시 한번 나오지만, 언어 모델은 "단어 시퀀스에 확률을 할당하는 모델"이다.
보편적으로는 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것이 있다고 한다.

조건부 확률이 이때 등장한다. 하나의 단어가 소문자 w, 단어 시퀀스는 대문자 W다. n개의 단어가 등장하는 단어 시퀀스 W의 확률은 다음과 같다.

``P(W) = P(w1, w2, w3, w4, w5, ..., wn)``

이를 조건부 확률로 나타내면 아래와 같다.

``P(wn|w1, ..., wn-1)``

``P(w5|w1, w2, w3, w4)``

전체 단어 시퀀스 W의 확률은 모든 단어가 예측되고 나서야 알 수 있기에, 새로운 식이 등장한다.
i=1에서 n까지의 모든 확률을 [곱집합](https://ko.wikipedia.org/wiki/%EA%B3%B1%EC%A7%91%ED%95%A9) 연산해야 단어 시퀀스의 확률을 알 수 있다.

### 4. 언어 모델의 간단한 직관
> 앞에 어떤 단어들이 나왔는지 고려하여 후보가 될 수 있는 여러 단어들에 대해서 확률을 예측해보고 가장 높은 확률을 가진 단어를 선택

## 2) 통계적 언어 모델
전통적인 접근 방법이며, Statistical Language Model을 줄여서 SLM이라 부른다.

### 1. 조건부 확률
사회조사분석사부터 경영통계학까지, 조건부 확률은 꾸준히 얼굴을 비춘다.

[확률 용어 정리 - 조건부확률](https://velog.io/@du-du-zi/%ED%99%95%EB%A5%A0-%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-%EC%A1%B0%EA%B1%B4%EB%B6%80%ED%99%95%EB%A5%A0)

``p(B|A) = P(A, B) / P(A)``
``A하에서 B일 확률 = A와 B가 교집합일 확률 / A일 확률``

위 식을 계산하기 위해서는 확률의 곱셈법칙도 필요하다.

``P(A, B) = P(A) * P(B|A)``
``A와 B의 교집합일 확률 = A일 확률 * A 하에서 B일 확률``

더 많은 확률들이 모이면 어떻게 될까? 조건부 확률의 연쇄법칙이 있다.

``P(x1, x2, x3, ..., xn) = P(x1) * P(x2|x1) * P(x3|x1, x2) * ... P(xn|x1, ..., xn-1)``

### 2. 문장에 대한 확률
위에서는 n개의 x들로 조건부 확률을 정의했다. 그렇다면 n개의 단어들이 있는 문장으로 x를 대체해보면 어떨까?
> An adorable little boy is spreading smiles

예시로 제시된 문장은 다음과 같다. 이를 확률로 나타내는 것까지는 간단하다.

``P(An adorable little boy is spreading smiles)``

문장에서 하나의 단어들은 아무런 이유 없이 그 자리를 차지하고 있지 않는다.
마치 사회의 관습, 문화처럼 각 단어들로 환원될 수 없는 무언가가 더 있기에 문장이 완성된다.
그렇기에 "문장의 확률을 구하고자 조건부 확률을 사용"하는 것이다.

> 앞서 언급한 조건부 확률의 일반화 식을 문장의 확률 관점에서 다시 적어보면 **문장의 확률**은 **각 단어들이 이전 단어**가 주어졌을 때 **다음 단어로 등장할 확률의 곱**으로 구성됩니다.

아까 전에 보았던 곱집합이 다시 등장한다. 조건부 확률과 곱집합이 합쳐진 수식으로, n=1부터 n까지 각각의 wn의 조건부 확률을 모두 곱한 셈이 된다.
최종적으로 각 단어들에 대입해보면 다음과 같다.

``P(An adorable little boy is spreading smiles) =
P(An) * P(adorable|An) * P(little|An adorable) * P(boy|An adorable little) *
... * (smiles|An adorable little boy is spreading)
``

중간에 생략된 부분이 있지만, 이런 형태로 조건부 확률의 연쇄 법칙을 활용한다.

### 3. 카운트 기반의 접근
위의 방법을 실제로 SLM이 어떻게 활용하고 있을까? SLM은 "카운트에 기반하여 확률을 계산"한다.

``P(is|An adorable little boy) = count(An adorable little boy is) / count(An adorable little boy)``

"An adorable little boy is"가 30번 등장했고, "An adorable little boy"가 100번 등장했다고 가정해보자.
이때 위 수식을 따라간다면 ``30 / 100 = 0.3 = 30%`` is가 등장할 조건부 확률은 30%가 된다.

### 4. 카운트 기반 접근의 한계 - 희소 문제
여기서 통계의 근본적인 문제가 등장한다. 바로 '근사'다.
통계에서 사용하는 여러 기법들은 주로 표본집단을 통해 모집단을 이해하려 노력한다.
중요한 점은 정확히 모집단과 동일한 값을 알아낼 수가 없다는 것이다. 따라서 최대한 근접한 값을 주로 찾게 된다.
자연어처리 역시 마찬가지다.

> 기계에게 많은 코퍼스를 훈련시켜서 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 언어 모델의 목표

즉, 미리 확보된 코퍼스(표본집단)를 통해 현실세계에서 사람들이 말하는 바(모집단)에 가까이 다가가고자 한다.
그러나 모집단을 전수조사 하는 것은 사실상 불가능에 가깝다. 마찬가지로 코퍼스를 수집하는 데에도 한계가 있을 수밖에 없다.

만약 수집한 말뭉치에서 "An adorable little boy"라는 문장이 없다면 분모가 0이 되어버리므로 수식 자체가 성립하지 않는다.
현실에서는 충분히 성립할 수 있는 문장인데도 불구하고 말이다. 통계라면 이를 제2종 오류라고 정리했을 것이다. 

``H0: An adorable little boy 다음에는 is가 나오지 않는다.``

``H1: An adorable little boy 다음에는 is가 나온다.``

귀무가설이 거짓임에도 불구하고 이를 기각하는데에 실패했으므로 제2종 오류에 해당하기도 한다.

아무튼 희소 문제는 다음과 같이 정의할 수 있다.

> 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제

이를 해결하기 위한 방법으로 n-gram 언어 모델(본 교재에서 다룸), 스무딩, 백오프와 같은 "일반화" 기법들이 있다.
그러나 희소 문제에 대한 근본적인 해결책을 제시하지 못하고, 통계적 언어 모델에서 인공 신경망 모델로 넘어간다.

## 3) N-gram 언어 모델
